| 一 | 二 | 三  | 四 |
|:--:|:--:|:--:|:--:|
|[CV算法](#CV)|[搜索广告推荐算法](#SearchAdvertiseRecommend)|[计算机基础](#BasicComputerKnowledge)|[编程基础](#BasicProgrammingKnowledge)

<h2 id="CV"></h2>

* **CV算法**
    * 1：[为什么引入非线性激活函数](#为什么引入非线性激活函数)
    * 2：[为什么使用ReLU激活函数](#为什么使用ReLU激活函数)
    * 3：[权重初始化](#权重初始化)
    * 4：[Unet网络和Vnet网络的区别](#Unet网络和Vnet网络的区别)
    * 5：[简述CNN和FCN的区别](#简述CNN和FCN的区别)
    * 6：[解释随机梯度下降SGD](#解释随机梯度下降SGD)
    * 7：[正则化的意义](#正则化的意义)
    * 8：[减少过拟合的方法](#减少过拟合的方法)
    * 9：[解释最大似然估计和最小二乘法](#解释最大似然估计和最小二乘法) ---imp
    * 10：[输入数据归一化的原因](#输入数据归一化的原因)
    * 11：[图像分割需要处理类别不平衡吗](#图像分割需要处理类别不平衡吗)
    * 11：[batch size](#batchsize)
    * 11：[比较dice loss 和 cross-entropy loss，说明优缺点](#比较diceloss和cross-entropyloss说明优缺点)


    



<h2 id="SearchAdvertiseRecommend"></h2>

* **搜索广告推荐算法**
    * [a](#a)
    
<h2 id="BasicComputerKnowledge"></h2>

* **计算机基础**
    * [b](#b)
    
<h2 id="BasicProgrammingKnowledge"></h2>

* **编程基础**
    * [C,C++程序编译的内存分配情况有哪几种并简要解释](#CCpp程序编译的内存分配情况有哪几种并简要解释)

> 稳定性：相同的元素在排序前和排序后的前后位置是否发生改变，没有改变则排序是稳定的，改变则排序是不稳定的 [——八大排序算法的稳定性](https://www.cnblogs.com/codingmylife/archive/2012/10/21/2732980.html)

<br>
<br>

## 1.为什么引入非线性激活函数

如果不用激励函数，在这种情况下你每一层输出都是上层输入的线性函数，很容易验证，
无论你神经网络有多少层，输出都是输入的线性组合，与没有隐藏层效果相当，这种情况就
是最原始的感知机（ Perceptron）了。正因为上面的原因，我们决定引入非线性函数作为激
励函数，这样深层神经网络就有意义了（不再是输入的线性组合，可以逼近任意函数）。

<br>

## 为什么使用 ReLU 激活函数

第一，采用 sigmoid 等函数，算激活函数时（指数运算），计算量大，反向传播求误差梯
度时，求导涉及除法，计算量相对大，而采用 Relu 激活函数，整个过程的计算量节省很多。

第二，对于深层网络， sigmoid 函数反向传播时，很容易就会出现梯度消失的情况（在
sigmoid 接近饱和区时，变换太缓慢，导数趋于 0，这种情况会造成信息丢失），从而无法完
成深层网络的训练。

第三， Relu 会使一部分神经元的输出为 0，这样就造成了网络的稀疏性，并且减少了参数
的相互依存关系，缓解了过拟合问题的发生。


<br>

## 权重初始化

***错误***：全零初始化： 因为如果网络中的每个神经元都计算出同样的输出，然后它们就会在反
向传播中计算出同样的梯度，从而进行同样的参数更新。 换句话说，如果权重被初始化为同
样的值，神经元之间就失去了不对称性的源头。

***小随机数初始化***： 因此，权重初始值要非常接近 0 又不能等于 0。解决方法就是将权重初始
化为很小的数值，以此来打破对称性。其思路是：如果神经元刚开始的时候是随机且不相等
的，那么它们将计算出不同的更新，并将自身变成整个网络的不同部分。小随机数权重初始
化的实现方法是： W = 0.01 * np.random.randn(D,H)。其中 randn 函数是基于零均值和标准差
的一个高斯分布来生成随机数的。根据这个式子，每个神经元的权重向量都被初始化为一个
随机向量，而这些随机向量又服从一个多变量高斯分布，这样在输入空间中，所有的神经元
的指向是随机的。也可以使用均匀分布生成的随机数，但是从实践结果来看，对于算法的结
果影响极小。

***使用 1/sqrt(n)校准方差***： 随着输入数据量的增长，随机初始化的神经元的输出数据的分布中
的方差也在增大。我们可以除以输入数据量的平方根来调整其数值范围，这样神经元输出的
方差就归一化到 1 了。也就是说，建议将神经元的权重向量初始化为： w = np.random.randn(n)
/ sqrt(n)。其中 n 是输入数据的数量。这样就保证了网络中所有神经元起始时有近似同样的
输出分布。实践经验证明，这样做可以提高收敛的速度。

<br>

## Unet网络和Vnet网络的区别

**Unet 网络**：可以分为两个部分，第一个部分对图像进行下采样，第二个部分对图像进行上采样。共做了四次下采样和四次上采样，下采样采用 maxpool 的方法（最大值池化），上
采样采用反卷积的方法，在每次采样之前对图像做两次卷积，没有 padding，因此图像在卷
积过程中大小在缩小。在下采样过程中，通道数是不断翻倍，上采样过程，通道数是不断减
半。同时将相对应的下采样过程中的特征图裁剪添加至上采样的特征图中，来弥补分割的细
节度。 Unet 网络在卷积的过程中采样激活函数为 ReLU

**Vnet 网络**：可以分为两部分部分，第一个部分对图像进行下采样，第二个部分对图像进行
上采样。共做了四次下采样和四次上采样，下采样采用卷积的方法，步长为 2，因此图像大
小减半（相比于池化的方法，使用卷积下采样，内存占用更小，因为在反向传播的时候最大
值池化需要存储最大值所在的位置）上采样采用反卷积的方法，在每次采样之前对图像做卷
积，使用了 padding，图像大小不变。在下采样过程中，通道数是不断翻倍，上采样过程，
通道数是不断减半。同时将相对应的下采样过程中的特征图添加至上采样的特征图中，来弥
补分割的细节度。 Vnet 网络在卷积的过程中采样激活函数为 PReLU

<br>

## 简述CNN和FCN的区别

**CNN 网络的目的在于分类。** 在末端是一个全连接层，将原来二维特征图转换成一维的固定
长度的特征向量，这就丢失了空间信息，最后输出一个特定长度的向量，表示输入图像属于
每一类的概率，以此作为分类的标签。

**FCN 网络的目的在于语义分割。** FCN 在结构上将 CNN 网络最后的全连接层变成卷积层。
FCN 可以接受任意尺寸的输入图像。通过反卷积层对最后一个卷积层的 feature map 进行上
采样, 使它恢复到输入图像相同的尺寸，从而可以对每个像素都产生了一个预测, 同时保留
了原始输入图像中的空间信息, 最后在与输入图等大小的特征图上对每个像素进行分类，逐
像素地用 softmax 分类计算损失,相当于每个像素对应一个训练样本

<br>

## 解释随机梯度下降SGD

随机抽取一批样本，利用现有参数对该样本的每一个输入生成一个估计输出 y，然后跟实际输出比较，
统计该批样本的误差，求平均以后得到平均误差，以此来作为更新参数的依据。

由于是抽取,因此不可避免的,得到的梯度肯定有误差.因此学习速率需要逐渐减小.否则模
型无法收敛 。因为误差,所以每一次迭代的梯度受抽样的影响比较大,也就是说梯度含有比较
大的噪声,不能很好的反映真实梯度.

在实践中，一般采用 SGD+momentum 的配置，相比普通的 SGD 方法，这种配置通常能极
大地加快收敛速度。虽然名字为动量，其物理意义更接近于摩擦，其可以降低速度值，降低
了系统的动能，防止石头在山谷的最底部不能停止情况的发生。动量的取值范围通常为[0.5,
0.9, 0.95, 0.99]，一种常见的做法是在迭代开始时将其设为 0.5，在一定的迭代次数（ epoch）
后，将其值更新为 0.99。

在算法迭代过程中逐步降低学习率（ step_size）通常可以加快算法的收敛速度。常用的用
来更新学习率的方法有三种：逐步降低（ Step decay），即经过一定迭代次数后将学习率乘
以一个小的衰减因子；指数衰减（ Exponential decay）；倒数衰减（ 1/t decay）。实践中发现
逐步衰减的效果优于另外两种方法，一方面在于其需要设置的超参数数量少，另一方面其可
解释性也强于另两种方法。

<br>

## 正则化的意义

正则化是用来降低 overfitting（过拟合）的，减少过拟合的的其他方法有：增加训练集数
量，等等。对于数据集梳理有限的情况下，防止过拟合的另外一种方式就是降低模型的复杂
度，怎么降低?一种方式就是在 loss 函数中加入正则化项，正则化项可以理解为复杂度， loss
越小越好，但 loss 加上正则项之后，为了使 loss 小，就不能让正则项变大，也就是不能让模
型更复杂，这样就降低了模型复杂度，也就降低了过拟合。这就是正则化。正则化也有很多
种，常见为两种 L2 和 L1。

<br>

## 减少过拟合的方法

一、 引入正则化

二、 Dropout

三、 提前终止训练

四、 增加样本量

五、 Batch normalization六、 Bagging 和其他集成方法（正则化）

七、 辅助分类节点（正则化）

八、 参数绑定和参数共享

<br>

## 解释最大似然估计和最小二乘法

**最大似然估计**：现在已经拿到了很多个样本（你的数据集中所有因变量），这
些样本值已经实现，最大似然估计就是去找到那个（组）参数估计值，使得前
面已经实现的样本值发生概率最大。因为你手头上的样本已经实现了，其发生
概率最大才符合逻辑。这时是求样本所有观测的联合概率最大化，是个连乘
积，只要取对数，就变成了线性加总。此时通过对参数求导数，并令一阶导数
为零，就可以通过解方程（组），得到最大似然估计值。

**最小二乘**：找到一个（组）估计值，使得实际值与估计值的距离最小。本来用
两者差的绝对值汇总并使之最小是最理想的，但绝对值在数学上求最小值比较
麻烦，因而替代做法是，找一个（组）估计值，使得实际值与估计值之差的平
方加总之后的值最小，称为最小二乘。 “ 二乘” 的英文为 least square，其实
英文的字面意思是“ 平方最小” 。这时，将这个差的平方的和式对参数求导
数，并取一阶导数为零，就是 OLSE。

<br>

## 输入数据归一化的原因

原因在于神经网络学习过程本质就是为了学习数据分布，一旦训练数据与测试数据的分布
不同，那么网络的泛化能力也大大降低；另外一方面，一旦每批训练数据的分布各不相同
(batch 梯度下降)，那么网络就要在每次迭代都去学习适应不同的分布，这样将会大大降低
网络的训练速度，这也正是为什么我们需要对数据都要做一个归一化预处理的原因。


<br>

## 图像分割需要处理类别不平衡吗

如果采用交叉熵来作为 loss，图像分割问题可以理解为像素级的分类问题，其肯定需要处
理类别不平衡的问题。统计目标分割图中类别的分布概率加入到最后的交叉熵中，这可理解
为一种“再缩放”的方式来处理像素级的类别不平衡。如果使用 dice 系数作为 loss，那图像
分割在我的理解上，就是考察输出结果和目标的符合度， diceloss 就不需要处理类别不平衡
了，毕竟类别不平衡是处理分类问题


<br>

## batchsize

一、在合理范围内，增大 Batch_Size 有何好处？

内存利用率提高了，大矩阵乘法的并行化效率提高。

跑完一次 epoch（全数据集）所需的迭代次数减少，对于相同数据量的处理速度进一步加
快。

在一定范围内，一般来说 Batch_Size 越大，其确定的下降方向越准，引起训练震荡越小。

二、盲目增大 Batch_Size 有何坏处？

内存利用率提高了，但是内存容量可能撑不住了。

跑完一次 epoch（全数据集）所需的迭代次数减少，要想达到相同的精度，其所花费的时
间大大增加了，从而对参数的修正也就显得更加缓慢。

Batch_Size 增大到一定程度，其确定的下降方向已经基本不再变化。

<br>

## 比较diceloss和cross-entropyloss说明优缺点

采用 cross-entropy loss 而不是采用 dice loss，因为 cross-entropy loss 的梯度好， dice loss 的
梯度相比 cross-entropy loss 难以计算，并且容易梯度爆炸，训练的时候不稳定。

采用 dice loss 的主要原因是为了直接优化评价指标，而 cross-entropy loss 只是一个代理，
但是可以最大化利用反向传播。另外， dice loss 在类别不平衡的问题表现得更好

* **二.搜索广告推荐算法**

## a

a
<br>

* **三.计算机基础**

## b

b



* **四.编程基础**

## CCpp程序编译的内存分配情况有哪几种并简要解释

一、从静态存储区域分配：
内存在程序编译时就已经分配好，这块内存在程序的整个运行期间都存在。速度快，不容
易出错，因为有系统会善后。例如全局变量， static 变量等。

二、在栈上分配：
在执行函数时,函数内局部变量的存储单元都在栈上创建，函数执行结束时这些存储单元
自动被释放。栈内存分配运算内置于处理器的指令集中，效率很高，但是分配的内存容量有
限。

三、从堆上分配：
即动态内存分配。程序在运行的时候用 malloc 或 new 申请任意大小的内存，程序员自己
负责在何时用 free 或 delete 释放内存。动态内存的生存期由程序员决定，使用非常灵活。
如果在堆上分配了空间,就有责任回收它，否则运行的程序会出现内存泄漏，另外频繁地分
配和释放不同大小的堆空间将会产生堆内碎块。

