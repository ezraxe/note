| 一 | 二 | 三  | 四 |
|:--:|:--:|:--:|:--:|
|[CV算法](#CV)|[搜索广告推荐算法](#SearchAdvertiseRecommend)|[计算机基础](#BasicComputerKnowledge)|[编程基础](#BasicProgrammingKnowledge)

<h2 id="CV"></h2>

* **CV算法**
    * [为什么引入非线性激活函数](#为什么引入非线性激活函数)
    * [为什么使用ReLU激活函数](#为什么使用ReLU激活函数)
    * [权重初始化](#权重初始化)
    * [Unet网络和Vnet网络的区别](#Unet网络和Vnet网络的区别)
    * [归并排序](#归并排序)
    * [堆排序](#堆排序)

<h2 id="SearchAdvertiseRecommend"></h2>

* **搜索广告推荐算法**
    * [a](#a)
    
<h2 id="BasicComputerKnowledge"></h2>

* **计算机基础**
    * [b](#b)
    
<h2 id="BasicProgrammingKnowledge"></h2>

* **编程基础**
    * [CCpp程序编译的内存分配情况有哪几种并简要解释](#CCpp程序编译的内存分配情况有哪几种并简要解释)

> 稳定性：相同的元素在排序前和排序后的前后位置是否发生改变，没有改变则排序是稳定的，改变则排序是不稳定的 [——八大排序算法的稳定性](https://www.cnblogs.com/codingmylife/archive/2012/10/21/2732980.html)

<br>
<br>

## 1.为什么引入非线性激活函数

如果不用激励函数，在这种情况下你每一层输出都是上层输入的线性函数，很容易验证，
无论你神经网络有多少层，输出都是输入的线性组合，与没有隐藏层效果相当，这种情况就
是最原始的感知机（ Perceptron）了。正因为上面的原因，我们决定引入非线性函数作为激
励函数，这样深层神经网络就有意义了（不再是输入的线性组合，可以逼近任意函数）。

<br>

## 为什么使用 ReLU 激活函数

第一，采用 sigmoid 等函数，算激活函数时（指数运算），计算量大，反向传播求误差梯
度时，求导涉及除法，计算量相对大，而采用 Relu 激活函数，整个过程的计算量节省很多。

第二，对于深层网络， sigmoid 函数反向传播时，很容易就会出现梯度消失的情况（在
sigmoid 接近饱和区时，变换太缓慢，导数趋于 0，这种情况会造成信息丢失），从而无法完
成深层网络的训练。

第三， Relu 会使一部分神经元的输出为 0，这样就造成了网络的稀疏性，并且减少了参数
的相互依存关系，缓解了过拟合问题的发生。


<br>

## 权重初始化

***错误***：全零初始化： 因为如果网络中的每个神经元都计算出同样的输出，然后它们就会在反
向传播中计算出同样的梯度，从而进行同样的参数更新。 换句话说，如果权重被初始化为同
样的值，神经元之间就失去了不对称性的源头。

***小随机数初始化***： 因此，权重初始值要非常接近 0 又不能等于 0。解决方法就是将权重初始
化为很小的数值，以此来打破对称性。其思路是：如果神经元刚开始的时候是随机且不相等
的，那么它们将计算出不同的更新，并将自身变成整个网络的不同部分。小随机数权重初始
化的实现方法是： W = 0.01 * np.random.randn(D,H)。其中 randn 函数是基于零均值和标准差
的一个高斯分布来生成随机数的。根据这个式子，每个神经元的权重向量都被初始化为一个
随机向量，而这些随机向量又服从一个多变量高斯分布，这样在输入空间中，所有的神经元
的指向是随机的。也可以使用均匀分布生成的随机数，但是从实践结果来看，对于算法的结
果影响极小。

***使用 1/sqrt(n)校准方差***： 随着输入数据量的增长，随机初始化的神经元的输出数据的分布中
的方差也在增大。我们可以除以输入数据量的平方根来调整其数值范围，这样神经元输出的
方差就归一化到 1 了。也就是说，建议将神经元的权重向量初始化为： w = np.random.randn(n)
/ sqrt(n)。其中 n 是输入数据的数量。这样就保证了网络中所有神经元起始时有近似同样的
输出分布。实践经验证明，这样做可以提高收敛的速度。

<br>

## Unet网络和Vnet网络的区别

Unet 网络：可以分为两个部分，第一个部分对图像进行下采样，第二个部分对图像进行上采样。共做了四次下采样和四次上采样，下采样采用 maxpool 的方法（最大值池化），上
采样采用反卷积的方法，在每次采样之前对图像做两次卷积，没有 padding，因此图像在卷
积过程中大小在缩小。在下采样过程中，通道数是不断翻倍，上采样过程，通道数是不断减
半。同时将相对应的下采样过程中的特征图裁剪添加至上采样的特征图中，来弥补分割的细
节度。 Unet 网络在卷积的过程中采样激活函数为 ReLU

Vnet 网络：可以分为两部分部分，第一个部分对图像进行下采样，第二个部分对图像进行
上采样。共做了四次下采样和四次上采样，下采样采用卷积的方法，步长为 2，因此图像大
小减半（相比于池化的方法，使用卷积下采样，内存占用更小，因为在反向传播的时候最大
值池化需要存储最大值所在的位置）上采样采用反卷积的方法，在每次采样之前对图像做卷
积，使用了 padding，图像大小不变。在下采样过程中，通道数是不断翻倍，上采样过程，
通道数是不断减半。同时将相对应的下采样过程中的特征图添加至上采样的特征图中，来弥
补分割的细节度。 Vnet 网络在卷积的过程中采样激活函数为 PReLU

<br>

## 6.归并排序

将一个序列分成两个长度相等的子序列，为每一个子序列排序，然后再将它们合并成一个序列。合并两个子序列的过程称为归并


<br>

## 7.堆排序

堆排序首先根据数组构建最大堆，然后每次“删除”堆顶元素（将堆顶元素移至末尾）。最后得到的序列就是从小到大排序的序列


<br>

* **二.搜索广告推荐算法**

## a

a

* **三.计算机基础**

## b

b



* **四.编程基础**

## CCpp程序编译的内存分配情况有哪几种并简要解释

一、从静态存储区域分配：
内存在程序编译时就已经分配好，这块内存在程序的整个运行期间都存在。速度快，不容
易出错，因为有系统会善后。例如全局变量， static 变量等。

二、在栈上分配：
在执行函数时,函数内局部变量的存储单元都在栈上创建，函数执行结束时这些存储单元
自动被释放。栈内存分配运算内置于处理器的指令集中，效率很高，但是分配的内存容量有
限。

三、从堆上分配：
即动态内存分配。程序在运行的时候用 malloc 或 new 申请任意大小的内存，程序员自己
负责在何时用 free 或 delete 释放内存。动态内存的生存期由程序员决定，使用非常灵活。
如果在堆上分配了空间,就有责任回收它，否则运行的程序会出现内存泄漏，另外频繁地分
配和释放不同大小的堆空间将会产生堆内碎块。

