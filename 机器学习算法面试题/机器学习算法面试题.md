| 一 | 二 | 三  | 四 |
|:--:|:--:|:--:|:--:|
|[CV算法](#CV)|[搜索广告推荐算法](#SearchAdvertiseRecommend)|[计算机基础](#BasicComputerKnowledge)|[编程基础](#BasicProgrammingKnowledge)

<h2 id="CV"></h2>

* **CV算法**
    * 1：[为什么引入非线性激活函数](#为什么引入非线性激活函数)
    * 2：[为什么使用ReLU激活函数](#为什么使用ReLU激活函数)
    * 3：[权重初始化](#权重初始化)
    * 4：[Unet网络和Vnet网络的区别](#Unet网络和Vnet网络的区别)
    * 5：[简述CNN和FCN的区别](#简述CNN和FCN的区别)
    * 6：[解释随机梯度下降SGD](#解释随机梯度下降SGD)
    * 7：[正则化的意义](#正则化的意义)
    * 8：[减少过拟合的方法](#减少过拟合的方法)
    * 9：[解释最大似然估计和最小二乘法](#解释最大似然估计和最小二乘法) ---imp
    * 10：[输入数据归一化的原因](#输入数据归一化的原因)
    * 11：[图像分割需要处理类别不平衡吗](#图像分割需要处理类别不平衡吗)
    * 11：[batch size](#batchsize)
    * 11：[比较dice loss 和 cross-entropy loss，说明优缺点](#比较diceloss和cross-entropyloss说明优缺点) ---imp
    * 11：[解释 batch normlization](#解释batchnormlization) ---imp
    * 10：[深度网络有什么好处](#深度网络有什么好处)
    * 10：[解释残差学习](#解释残差学习)
    * 10：[解释dropout](#解释dropout) ---imp
    * 10：[softmax 和 sigmoid 的关系和区别](#softmax和sigmoid的关系和区别) ---imp
    * 10：[解释 deconvolution](#解释deconvolution) ---imp
    * 10：[正则化](#正则化)
    * 10：[感受野](#感受野)
    * 10：[万能近似定理](#万能近似定理)
    * 10：[什麽样的数据集不适合用深度学习](#什麽样的数据集不适合用深度学习) 
    * 10：[CNN 最成功的应用是在 CV，那为什么 NLP 和 Speech 的很多问题也可以用 CNN 解出来？
为什么 AlphaGo 里也用了 CNN？这几个不相关的问题的相似性在哪里？ CNN 通过什么手段
抓住了这个共性？](#为什么NLP和Speech的很多问题也可以用CNN解出来?CNN通过什么手段抓住了这个共性？) 




    



<h2 id="SearchAdvertiseRecommend"></h2>

* **搜索广告推荐算法**
    * [a](#a)
    
<h2 id="BasicComputerKnowledge"></h2>

* **计算机基础**
    * [b](#b)
    
<h2 id="BasicProgrammingKnowledge"></h2>

* **编程基础**
    * [C,C++程序编译的内存分配情况有哪几种并简要解释](#CCpp程序编译的内存分配情况有哪几种并简要解释)

> 稳定性：相同的元素在排序前和排序后的前后位置是否发生改变，没有改变则排序是稳定的，改变则排序是不稳定的 [——八大排序算法的稳定性](https://www.cnblogs.com/codingmylife/archive/2012/10/21/2732980.html)

<br>
<br>

## 1.为什么引入非线性激活函数

如果不用激励函数，在这种情况下你每一层输出都是上层输入的线性函数，很容易验证，
无论你神经网络有多少层，输出都是输入的线性组合，与没有隐藏层效果相当，这种情况就
是最原始的感知机（ Perceptron）了。正因为上面的原因，我们决定引入非线性函数作为激
励函数，这样深层神经网络就有意义了（不再是输入的线性组合，可以逼近任意函数）。

<br>

## 为什么使用 ReLU 激活函数

第一，采用 sigmoid 等函数，算激活函数时（指数运算），计算量大，反向传播求误差梯
度时，求导涉及除法，计算量相对大，而采用 Relu 激活函数，整个过程的计算量节省很多。

第二，对于深层网络， sigmoid 函数反向传播时，很容易就会出现梯度消失的情况（在
sigmoid 接近饱和区时，变换太缓慢，导数趋于 0，这种情况会造成信息丢失），从而无法完
成深层网络的训练。

第三， Relu 会使一部分神经元的输出为 0，这样就造成了网络的稀疏性，并且减少了参数
的相互依存关系，缓解了过拟合问题的发生。


<br>

## 权重初始化

***错误***：全零初始化： 因为如果网络中的每个神经元都计算出同样的输出，然后它们就会在反
向传播中计算出同样的梯度，从而进行同样的参数更新。 换句话说，如果权重被初始化为同
样的值，神经元之间就失去了不对称性的源头。

***小随机数初始化***： 因此，权重初始值要非常接近 0 又不能等于 0。解决方法就是将权重初始
化为很小的数值，以此来打破对称性。其思路是：如果神经元刚开始的时候是随机且不相等
的，那么它们将计算出不同的更新，并将自身变成整个网络的不同部分。小随机数权重初始
化的实现方法是： W = 0.01 * np.random.randn(D,H)。其中 randn 函数是基于零均值和标准差
的一个高斯分布来生成随机数的。根据这个式子，每个神经元的权重向量都被初始化为一个
随机向量，而这些随机向量又服从一个多变量高斯分布，这样在输入空间中，所有的神经元
的指向是随机的。也可以使用均匀分布生成的随机数，但是从实践结果来看，对于算法的结
果影响极小。

***使用 1/sqrt(n)校准方差***： 随着输入数据量的增长，随机初始化的神经元的输出数据的分布中
的方差也在增大。我们可以除以输入数据量的平方根来调整其数值范围，这样神经元输出的
方差就归一化到 1 了。也就是说，建议将神经元的权重向量初始化为： w = np.random.randn(n)
/ sqrt(n)。其中 n 是输入数据的数量。这样就保证了网络中所有神经元起始时有近似同样的
输出分布。实践经验证明，这样做可以提高收敛的速度。

<br>

## Unet网络和Vnet网络的区别

**Unet 网络**：可以分为两个部分，第一个部分对图像进行下采样，第二个部分对图像进行上采样。共做了四次下采样和四次上采样，下采样采用 maxpool 的方法（最大值池化），上
采样采用反卷积的方法，在每次采样之前对图像做两次卷积，没有 padding，因此图像在卷
积过程中大小在缩小。在下采样过程中，通道数是不断翻倍，上采样过程，通道数是不断减
半。同时将相对应的下采样过程中的特征图裁剪添加至上采样的特征图中，来弥补分割的细
节度。 Unet 网络在卷积的过程中采样激活函数为 ReLU

**Vnet 网络**：可以分为两部分部分，第一个部分对图像进行下采样，第二个部分对图像进行
上采样。共做了四次下采样和四次上采样，下采样采用卷积的方法，步长为 2，因此图像大
小减半（相比于池化的方法，使用卷积下采样，内存占用更小，因为在反向传播的时候最大
值池化需要存储最大值所在的位置）上采样采用反卷积的方法，在每次采样之前对图像做卷
积，使用了 padding，图像大小不变。在下采样过程中，通道数是不断翻倍，上采样过程，
通道数是不断减半。同时将相对应的下采样过程中的特征图添加至上采样的特征图中，来弥
补分割的细节度。 Vnet 网络在卷积的过程中采样激活函数为 PReLU

<br>

## 简述CNN和FCN的区别

**CNN 网络的目的在于分类。** 在末端是一个全连接层，将原来二维特征图转换成一维的固定
长度的特征向量，这就丢失了空间信息，最后输出一个特定长度的向量，表示输入图像属于
每一类的概率，以此作为分类的标签。

**FCN 网络的目的在于语义分割。** FCN 在结构上将 CNN 网络最后的全连接层变成卷积层。
FCN 可以接受任意尺寸的输入图像。通过反卷积层对最后一个卷积层的 feature map 进行上
采样, 使它恢复到输入图像相同的尺寸，从而可以对每个像素都产生了一个预测, 同时保留
了原始输入图像中的空间信息, 最后在与输入图等大小的特征图上对每个像素进行分类，逐
像素地用 softmax 分类计算损失,相当于每个像素对应一个训练样本

<br>

## 解释随机梯度下降SGD

随机抽取一批样本，利用现有参数对该样本的每一个输入生成一个估计输出 y，然后跟实际输出比较，
统计该批样本的误差，求平均以后得到平均误差，以此来作为更新参数的依据。

由于是抽取,因此不可避免的,得到的梯度肯定有误差.因此学习速率需要逐渐减小.否则模
型无法收敛 。因为误差,所以每一次迭代的梯度受抽样的影响比较大,也就是说梯度含有比较
大的噪声,不能很好的反映真实梯度.

在实践中，一般采用 SGD+momentum 的配置，相比普通的 SGD 方法，这种配置通常能极
大地加快收敛速度。虽然名字为动量，其物理意义更接近于摩擦，其可以降低速度值，降低
了系统的动能，防止石头在山谷的最底部不能停止情况的发生。动量的取值范围通常为[0.5,
0.9, 0.95, 0.99]，一种常见的做法是在迭代开始时将其设为 0.5，在一定的迭代次数（ epoch）
后，将其值更新为 0.99。

在算法迭代过程中逐步降低学习率（ step_size）通常可以加快算法的收敛速度。常用的用
来更新学习率的方法有三种：逐步降低（ Step decay），即经过一定迭代次数后将学习率乘
以一个小的衰减因子；指数衰减（ Exponential decay）；倒数衰减（ 1/t decay）。实践中发现
逐步衰减的效果优于另外两种方法，一方面在于其需要设置的超参数数量少，另一方面其可
解释性也强于另两种方法。

<br>

## 正则化的意义

正则化是用来降低 overfitting（过拟合）的，减少过拟合的的其他方法有：增加训练集数
量，等等。对于数据集梳理有限的情况下，防止过拟合的另外一种方式就是降低模型的复杂
度，怎么降低?一种方式就是在 loss 函数中加入正则化项，正则化项可以理解为复杂度， loss
越小越好，但 loss 加上正则项之后，为了使 loss 小，就不能让正则项变大，也就是不能让模
型更复杂，这样就降低了模型复杂度，也就降低了过拟合。这就是正则化。正则化也有很多
种，常见为两种 L2 和 L1。

<br>

## 减少过拟合的方法

一、 引入正则化

二、 Dropout

三、 提前终止训练

四、 增加样本量

五、 Batch normalization六、 Bagging 和其他集成方法（正则化）

七、 辅助分类节点（正则化）

八、 参数绑定和参数共享

<br>

## 解释最大似然估计和最小二乘法

**最大似然估计**：现在已经拿到了很多个样本（你的数据集中所有因变量），这
些样本值已经实现，最大似然估计就是去找到那个（组）参数估计值，使得前
面已经实现的样本值发生概率最大。因为你手头上的样本已经实现了，其发生
概率最大才符合逻辑。这时是求样本所有观测的联合概率最大化，是个连乘
积，只要取对数，就变成了线性加总。此时通过对参数求导数，并令一阶导数
为零，就可以通过解方程（组），得到最大似然估计值。

**最小二乘**：找到一个（组）估计值，使得实际值与估计值的距离最小。本来用
两者差的绝对值汇总并使之最小是最理想的，但绝对值在数学上求最小值比较
麻烦，因而替代做法是，找一个（组）估计值，使得实际值与估计值之差的平
方加总之后的值最小，称为最小二乘。 “ 二乘” 的英文为 least square，其实
英文的字面意思是“ 平方最小” 。这时，将这个差的平方的和式对参数求导
数，并取一阶导数为零，就是 OLSE。

<br>

## 输入数据归一化的原因

原因在于神经网络学习过程本质就是为了学习数据分布，一旦训练数据与测试数据的分布
不同，那么网络的泛化能力也大大降低；另外一方面，一旦每批训练数据的分布各不相同
(batch 梯度下降)，那么网络就要在每次迭代都去学习适应不同的分布，这样将会大大降低
网络的训练速度，这也正是为什么我们需要对数据都要做一个归一化预处理的原因。


<br>

## 图像分割需要处理类别不平衡吗

如果采用交叉熵来作为 loss，图像分割问题可以理解为像素级的分类问题，其肯定需要处
理类别不平衡的问题。统计目标分割图中类别的分布概率加入到最后的交叉熵中，这可理解
为一种“再缩放”的方式来处理像素级的类别不平衡。如果使用 dice 系数作为 loss，那图像
分割在我的理解上，就是考察输出结果和目标的符合度， diceloss 就不需要处理类别不平衡
了，毕竟类别不平衡是处理分类问题


<br>

## batchsize

一、在合理范围内，增大 Batch_Size 有何好处？

内存利用率提高了，大矩阵乘法的并行化效率提高。

跑完一次 epoch（全数据集）所需的迭代次数减少，对于相同数据量的处理速度进一步加
快。

在一定范围内，一般来说 Batch_Size 越大，其确定的下降方向越准，引起训练震荡越小。

二、盲目增大 Batch_Size 有何坏处？

内存利用率提高了，但是内存容量可能撑不住了。

跑完一次 epoch（全数据集）所需的迭代次数减少，要想达到相同的精度，其所花费的时
间大大增加了，从而对参数的修正也就显得更加缓慢。

Batch_Size 增大到一定程度，其确定的下降方向已经基本不再变化。

<br>

## 比较diceloss和cross-entropyloss说明优缺点

采用 cross-entropy loss 而不是采用 dice loss，因为 cross-entropy loss 的梯度好， dice loss 的
梯度相比 cross-entropy loss 难以计算，并且容易梯度爆炸，训练的时候不稳定。

采用 dice loss 的主要原因是为了直接优化评价指标，而 cross-entropy loss 只是一个代理，
但是可以最大化利用反向传播。另外， dice loss 在类别不平衡的问题表现得更好

<br>

## 解释batchnormlization

将一个 batch 的数据变换到均值为 0、方差为 1 的正态分布上，从而使数据分布一致，
每层的梯度不会随着网络结构的加深发生太大变化，从而避免发生梯度消失并且加快收敛，同时还有防止过拟合的效果；

随着输入数据的不断变化，以及网络中参数不断调整，网络的各层输入数据的分布则会不
断变化，那么各层在训练的过程中就需要不断的改变以适应这种新的数据分布，从而造成网
络训练困难，难以拟合的问题。 BN 算法解决的就是这样的问题，他通过对每一层的输入进
行归一化，保证每层的输入数据分布是稳定的，从而达到加速训练的目的.

![](../pic/ml-cv-bn-1.png)

![](../pic/ml-cv-bn-2.png)

最后在全体训练集上求取均值和方差（无偏估计），去取代模型 BN 变换层的对应均值和方
差。训练完成后的均值方差还只是最后一个 banch 的均值方差，我们最后的模型是基于训练
集的，所以最后还需要增加加一步替换操作。

![](../pic/ml-cv-bn-3.png)

BN 的反向传播推导：

![](../pic/ml-cv-bn-4.png)

<br>

## 深度网络有什么好处

特征的“等级”随着网络深度的加深而变高；极其深的深度使该网络拥有极强大的表达能力

<br>

## 解释残差学习

深度网络容易造成梯度在 back propagation 的过程中消失，导致训练效果很差，而深度残
差网络在神经网络的结构层面解决了这一问题，使得就算网络很深，梯度也不会消失。
对于神经网络来讲，我们需要通过反向传播来对网络的权重进行调整就像这样

![](../pic/ml-cv-resnet-1.png)

这个偏导就是我们求的 gradient，这个值本来就很小，而且再计算的时候还要再乘
stepsize，就更小了 所以通过这里可以看到，梯度在反向传播过程中的计算，如果 N 很
大，那么梯度值传播到前几层的时候就会越来越小，也就是梯度消失的问题。那 DRN 是怎
样解决这个问题的呢？它在神经网络结构的层面解决了这个问题 它将基本的单元改成了这
个样子

![](../pic/ml-cv-resnet-2.png)

这样就算深度很深，梯度也不会消失了。当然深度残差这篇文章最后的效果好，是因为
还结合了调参数以及神经网络的其他的细节，这些也很重要，不过就不是这里我们关心的
内容了。可以看到，对于相同的数据集来讲，残差网络比同等深度的其他网络表现出了更
好的性能，收敛更快。

<br>

## 解释dropout

Dropout 的目的也是用来减少 overfitting（过拟合）。而和 L1， L2Regularization 不同的
是， Dropout 不是针对 cost 函数，而是改变神经网络本身的结构。假设有一个神经网络：
按照之前的方法，根据输入 X，先正向更新神经网络，得到输出值，然后反向根据backpropagation 算法来更新权重和偏向。而 Dropout 不同的是，

1） 在开始，随机删除掉隐藏层一半（一定概率） 的神经元

2）然后，在删除后的剩下一半（一定概率） 的神经元上正向和反向更新权重和偏向；

3）再恢复之前删除的神经元，再重新随机删除一半（一定概率） 的神经元，进行正向和
反向更新 w 和 b;

4）重复上述过程。

最后，学习出来的神经网络中的每个神经元都是在只有一半（一定概率） 的神经元的基础
上学习的，因为更新次数减半，那么学习的权重会偏大，所以当所有神经元被回复后（上述
步骤 2）），把得到的隐藏层的权重减半（乘以概率） 。

对于 Dropout 为什么可以减少 overfitting 的原因如下：

一般情况下，对于同一组训练数据，利用不同的神经网络训练之后，求其输出的平均值
可以减少 overfitting。 Dropout 就是利用这个原理，每次丢掉一半的一隐藏层神经元，相当
于在不同的神经网络上进行训练，这样就减少了神经元之间的依赖性，即每个神经元不能
依赖于某几个其他的神经元（指层与层之间相连接的神经元），使神经网络更加能学习到
与其他神经元之间的更加健壮 robust 的特征。在 Dropout 的作者文章中，测试手写数字的
准确率达到了 98.7%!所以 Dropout 不仅减少 overfitting，还能提高准确率

![](../pic/ml-cv-dropout-1.png)

<br>

## softmax和sigmoid的关系和区别

softmax 是应对多分类问题， sigmoid 是应对二分类问题。 softmax 可以转换为 sigmoid。

<br>

## 解释deconvolution

![](../pic/ml-cv-deconv-1.png)

根据输出的大小和步长，卷积矩阵 C 采用不同的格式

<br>

## 正则化

正则化策略主要的目的是限制学习算法的能力，主要的方法可以是：限制网络模型的
神经元数量、限制模型参数（连接权重 W，偏置项 B 等）的数目、在目标函数添加一些额
外的惩罚项等。添加惩罚项可看成是对损失函数中的某些参数做一些限制，根据惩罚项的
不同可分为： L0 范数惩罚、 L1 范数惩罚（参数稀疏性惩罚）、 L2 范数惩罚（权重衰减惩
罚） 。

L0 范数是指向量中非 0 的元素的个数。

![](../pic/ml-cv-regularization-1.png)

L1 正则化可以产生稀疏权值矩阵，即产生一个稀疏模型，因此可以用于特征选择

L2 正则化可以防止模型过拟合（ overfitting）；

一定程度上， L1 也可以防止过拟合

Regularization 能降低 overfitting 的原因：

在神经网络中，正则化网络更倾向于小的权重，在权重小的情况下，数据随机的变化
不会对神经网络的模型造成太大的影响，所以可能性更小的受到数据局部噪音的影响。而
未加入正则化的神经网络，权重大，容易通过较大的模型改变来适应数据，更容易学习到
局部的噪音。

1.3 其它被视为正则化的策略

在深度学习中你听过的很多名词其实都是一种正则化策略，下面我们将介绍这些正则
化策略。

1.参数共享，在卷积神经网络中，卷积操作其实就采用了参数共享，通过参数共享，
减少模型参数，降低模型复杂度，从而减少模型过拟合风险，卷积神经网络的详细内容见
《卷积神经网络你需要知道的几个基本概念》。

2.噪声注入以及数据扩充，降低泛化错误率直接的方法就是训练更多的数据，但有监
督学习中，带标签的数据往往是有限的， 我们可以通过噪声注入以及数据扩充方法在现有
数据的基础上扩充数据集。

3.稀疏表征，这种正则化策略事通过某种惩罚措施来抑制神经网络隐藏层中大部分神
经元，当信息输入神经网络时，只有关键部分神经元处于激活状态。这和 1.2.2 节的 L1 范
数惩罚相似，只不过， L1 范数是使模型参数稀疏化，而表征稀疏化是隐藏层输出大多数为
零或接近零。

4.Dropout，以 dropout 概率随机断开神经元连接，是一种非常高效的深度学习正则化
措施。

<br>

## 感受野

定义： 感受野用来表示网络内部的不同神经元对原图像的感受范围的大小，或者说，
convNets(cnn)每一层输出的特征图(feature map)上的像素点在原始图像上映射的区域大小。

神经元之所以无法对原始图像的所有信息进行感知，是因为在这些网络结构中普遍使用卷
积层和 pooling 层，在层与层之间均为局部连接。

神经元感受野的值越大表示其能接触到的原始图像范围就越大，也意味着它可能蕴含更为
全局，语义层次更高的特征；相反，值越小则表示其所包含的特征越趋向局部和细节。因此
感受野的值可以用来大致判断每一层的抽象层次

![](../pic/ml-cv-receptiveField-1.png)

<br>

## 万能近似定理

万能近似定理表明，一个前馈神经网络如果具有线性输出层和至少一层具有任何一种‘挤
压’性质的激活函数（例如 logistic sigmoid 激活函数）的隐藏层，只要给予网络足够数量的
隐藏单元，它可以以任意的精度来近似任何从一个有限维空间到另一个有限维空间的 Borel
可测函数。

万能近似定理意味着无论我们试图学习什么函数，我们知道一个大的 MLP 一定能够表示
这个函数。然而，我们不能保证训练算法能够学得这个函数。即使 MLP 能够表示该函数，
学习也可能因两个不同的原因而失败。

1、用于训练的优化算法可能找不到用于期望函数的参数值。

2、训练算法可能由于过拟合而选择了错误的函数。

<br>

## 什麽样的数据集不适合用深度学习

+（1）数据集太小，数据样本不足时，深度学习相对其它机器学习算法，没有明显优势。

+（2）数据集没有局部相关特性，目前深度学习表现比较好的领域主要是图像／语音／自然
语言处理等领域，这些领域的一个共性是局部相关性。图像中像素组成物体，语音信号中音
位组合成单词，文本数据中单词组合成句子，这些特征元素的组合一旦被打乱，表示的含义
同时也被改变

<br>

## 为什么NLP和Speech的很多问题也可以用CNN解出来?CNN通过什么手段抓住了这个共性？

以上几个不相关问题的相关性在于，都存在局部与整体的关系，由低层次的特征经过组合，
组成高层次的特征，并且得到不同特征之间的空间相关性。

CNN 抓住此共性的手段主要有四个：局部连接／权值共享／池化操作／多层次结构。

局部连接使网络可以提取数据的局部特征；权值共享大大降低了网络的训练难度，一个
Filter 只提取一个特征，在整个图片（或者语音／文本） 中进行卷积；池化操作与多层次结
构一起，实现了数据的降维，将低层次的局部特征组合成为较高层次的特征，从而对整个图
片进行表示

<br>


* **二.搜索广告推荐算法**

## a

a

<br>

* **三.计算机基础**

## b

b



* **四.编程基础**

## CCpp程序编译的内存分配情况有哪几种并简要解释

一、从静态存储区域分配：
内存在程序编译时就已经分配好，这块内存在程序的整个运行期间都存在。速度快，不容
易出错，因为有系统会善后。例如全局变量， static 变量等。

二、在栈上分配：
在执行函数时,函数内局部变量的存储单元都在栈上创建，函数执行结束时这些存储单元
自动被释放。栈内存分配运算内置于处理器的指令集中，效率很高，但是分配的内存容量有
限。

三、从堆上分配：
即动态内存分配。程序在运行的时候用 malloc 或 new 申请任意大小的内存，程序员自己
负责在何时用 free 或 delete 释放内存。动态内存的生存期由程序员决定，使用非常灵活。
如果在堆上分配了空间,就有责任回收它，否则运行的程序会出现内存泄漏，另外频繁地分
配和释放不同大小的堆空间将会产生堆内碎块。

